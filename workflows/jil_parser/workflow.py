"""
JIL Parser Workflow

Analyzes Autosys JIL files to identify job dependencies.

Architecture: Iterative analysis with artifact storage
- Root agent initiates analysis
- Loop agent iteratively discovers dependencies
- Conditional logic determines when complete
"""

from langgraph.graph import StateGraph, START, END
from typing import TypedDict, List, Dict, Any
from langchain_anthropic import ChatAnthropic
import os
from dotenv import load_dotenv
from core.base_workflow import BaseWorkflow, WorkflowMetadata, WorkflowInputParameter
from infrastructure.tools import get_s3_tools

# Load environment variables
load_dotenv()


class JILParserState(TypedDict):
    """State for JIL parser workflow"""
    file_path: str
    current_job: str
    dependencies: List[Dict[str, str]]
    visited_files: List[str]
    iteration_count: int
    max_iterations: int
    output: Dict[str, Any]


class JILParserWorkflow(BaseWorkflow):
    """
    Analyzes Autosys JIL files to identify job dependencies.

    Architecture: Iterative analysis with artifact storage
    - Root agent initiates analysis
    - Loop agent iteratively discovers dependencies
    - Conditional logic determines when complete
    """

    def __init__(self, config: Dict = None):
        """
        Initialize JIL Parser workflow.

        Args:
            config: Optional configuration dictionary
        """
        self.config = config or {}

        # Initialize LLM with S3 tools for file access
        self.s3_tools = get_s3_tools()
        self.llm = ChatAnthropic(
            model="claude-sonnet-4-20250514",
            api_key=os.getenv("ANTHROPIC_API_KEY"),
            temperature=0
        )

        # LLM with tools bound for agent operations
        self.llm_with_tools = self.llm.bind_tools(self.s3_tools)

    def get_metadata(self) -> WorkflowMetadata:
        """Return workflow metadata for registry discovery"""
        return WorkflowMetadata(
            name="jil_parser",
            description="Analyzes Autosys JIL files to identify upstream and downstream job dependencies",
            capabilities=[
                "Parse JIL file structure",
                "Identify upstream job dependencies (conditions)",
                "Identify downstream job dependencies (box jobs)",
                "Trace multi-level dependency chains",
                "Map job relationships"
            ],
            example_queries=[
                "Parse JIL dependencies for BATCH_PROCESSING job",
                "What are the upstream jobs for DATA_LOAD?",
                "Analyze the JIL file and show me all dependencies",
                "Identify downstream jobs for ETL_MASTER"
            ],
            category="migration",
            version="1.0.0",
            author="Data Engineering Team",
            required_inputs=[
                WorkflowInputParameter(
                    name="file_path",
                    description="Path to the JIL file to analyze",
                    type="file_path",
                    required=True,
                    example="/path/to/autosys/jobs.jil",
                    prompt="What is the path to the JIL file you want me to analyze?"
                ),
                WorkflowInputParameter(
                    name="current_job",
                    description="The name of the specific job to analyze for dependencies",
                    type="string",
                    required=True,
                    example="BATCH_PROCESSING_JOB",
                    prompt="Which job name should I analyze for dependencies?"
                ),
                WorkflowInputParameter(
                    name="max_iterations",
                    description="Maximum number of iterations for dependency discovery",
                    type="integer",
                    required=False,
                    default=3,
                    example="5",
                    prompt="How many iterations should I perform? (default: 3)"
                )
            ]
        )

    def get_compiled_graph(self):
        """Build JIL parser graph with iterative analysis"""

        def root_agent(state: JILParserState) -> Dict:
            """
            Initialize analysis.

            Uses S3 tools to read JIL file if path starts with s3://
            Otherwise treats as local simulation.

            Args:
                state: Current workflow state

            Returns:
                State updates with initial dependencies
            """
            file_path = state['file_path']
            current_job = state['current_job']

            # Check if file is in S3
            if file_path.startswith('s3://'):
                # Parse S3 path: s3://bucket/key
                parts = file_path.replace('s3://', '').split('/', 1)
                bucket = parts[0] if len(parts) > 0 else None
                key = parts[1] if len(parts) > 1 else None

                prompt = f"""You have access to S3 tools to read files.

Read the JIL file from S3 and analyze job: {current_job}
File location: {file_path}

Use the s3_read_file tool to read the file content, then identify:
1. Upstream dependencies (condition statements)
2. Downstream dependencies (box job memberships)

Return your findings in this format:
- Job name
- Upstream jobs (list with relationship type)
- Downstream jobs (list with relationship type)
"""

                # Invoke LLM with tools
                response = self.llm_with_tools.invoke(prompt)

                # Check if tools were called
                if hasattr(response, 'tool_calls') and response.tool_calls:
                    # LLM wants to use tools - execute them
                    for tool_call in response.tool_calls:
                        tool_name = tool_call['name']
                        tool_args = tool_call['args']

                        # Find and execute the tool
                        for tool in self.s3_tools:
                            if tool.name == tool_name:
                                tool_result = tool._run(**tool_args)
                                # In production, would pass result back to LLM
                                # For now, continue with analysis
                                break

            else:
                # Local file or simulation
                prompt = f"""Analyze JIL job: {current_job}
File: {file_path}

Note: File path is local (not S3). Simulating analysis.

Identify immediate dependencies (both upstream conditions and downstream box jobs).
Return a structured analysis."""

                response = self.llm.invoke(prompt)

            # Parse response or use simulated structure
            initial_deps = [
                {"job": current_job, "type": "target", "relation": "self"}
            ]

            return {
                "dependencies": initial_deps,
                "visited_files": [file_path],
                "iteration_count": 0
            }

        def loop_agent(state: JILParserState) -> Dict:
            """
            Iteratively discover dependencies.

            Args:
                state: Current workflow state

            Returns:
                State updates with new dependencies found
            """
            # Check iteration limit
            if state["iteration_count"] >= state["max_iterations"]:
                return {"iteration_count": state["iteration_count"]}

            # Analyze dependencies
            prompt = f"""Continue dependency analysis for JIL job.
Current dependencies found: {len(state['dependencies'])}
Iteration: {state['iteration_count'] + 1} of {state['max_iterations']}

Job: {state['current_job']}
Known dependencies: {state['dependencies']}

Find any new upstream or downstream dependencies not yet discovered.
Focus on condition statements and box job memberships."""

            response = self.llm.invoke(prompt)

            # Simulate finding new dependencies
            # In production, would parse from response and actually read JIL files
            new_deps = []

            # Simulate discovering dependencies based on iteration
            if state["iteration_count"] == 0:
                new_deps.append({
                    "job": "UPSTREAM_JOB_1",
                    "type": "upstream",
                    "relation": "condition"
                })
            elif state["iteration_count"] == 1:
                new_deps.append({
                    "job": "DOWNSTREAM_JOB_1",
                    "type": "downstream",
                    "relation": "box_member"
                })

            return {
                "dependencies": state["dependencies"] + new_deps,
                "iteration_count": state["iteration_count"] + 1
            }

        def check_completion(state: JILParserState) -> str:
            """
            Determine if analysis complete.

            Args:
                state: Current workflow state

            Returns:
                "finalize" to end, "continue" to keep iterating
            """
            if state["iteration_count"] >= state["max_iterations"]:
                return "finalize"

            # In production, check if all dependencies resolved
            # For demonstration, simulate completion after finding some dependencies
            if len(state["dependencies"]) >= 3:
                return "finalize"

            return "continue"

        def finalize(state: JILParserState) -> Dict:
            """
            Compile final results.

            Args:
                state: Current workflow state

            Returns:
                State update with final output
            """
            output = {
                "success": True,
                "result": {
                    "target_job": state["current_job"],
                    "dependencies": state["dependencies"],
                    "files_analyzed": state["visited_files"],
                    "dependency_count": len(state["dependencies"]),
                    "upstream_jobs": [d for d in state["dependencies"] if d.get("type") == "upstream"],
                    "downstream_jobs": [d for d in state["dependencies"] if d.get("type") == "downstream"]
                },
                "metadata": {
                    "workflow_name": "jil_parser",
                    "iterations": state["iteration_count"],
                    "total_dependencies": len(state["dependencies"]),
                    "files_analyzed": len(state["visited_files"])
                },
                "artifacts": [],
                "errors": None
            }
            return {"output": output}

        # Build graph
        graph = StateGraph(JILParserState)
        graph.add_node("root_agent", root_agent)
        graph.add_node("loop_agent", loop_agent)
        graph.add_node("finalize", finalize)

        graph.add_edge(START, "root_agent")
        graph.add_edge("root_agent", "loop_agent")
        graph.add_conditional_edges(
            "loop_agent",
            check_completion,
            {
                "continue": "loop_agent",
                "finalize": "finalize"
            }
        )
        graph.add_edge("finalize", END)

        return graph.compile()


# For backwards compatibility - create instance and expose compiled graph
_workflow_instance = JILParserWorkflow()
workflow_graph = _workflow_instance.get_compiled_graph()


# For testing
if __name__ == "__main__":
    # Test the workflow
    print("Testing JIL Parser Workflow")
    print("-" * 60)

    workflow = JILParserWorkflow()
    graph = workflow.get_compiled_graph()

    test_input = {
        "file_path": "/path/to/sample.jil",
        "current_job": "BATCH_PROCESSING_JOB",
        "dependencies": [],
        "visited_files": [],
        "iteration_count": 0,
        "max_iterations": 3,
        "output": {}
    }

    print(f"Analyzing job: {test_input['current_job']}")
    print(f"Max iterations: {test_input['max_iterations']}")
    print()

    result = graph.invoke(test_input)

    print("Analysis Complete!")
    print(f"Iterations performed: {result['iteration_count']}")
    print(f"Dependencies found: {len(result['output']['result']['dependencies'])}")
    print()
    print("Result:")
    import json
    print(json.dumps(result['output'], indent=2))
    print("-" * 60)
    print("âœ“ JIL Parser workflow test completed")
